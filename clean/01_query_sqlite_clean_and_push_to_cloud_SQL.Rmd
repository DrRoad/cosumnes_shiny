---
title: "Query SQLite, clean, and push to cloud SQL"
output: html_document
---

The purpose of this script is to:  

* obtain data from the Home Station Database for Level Sender, in `C:/Users/rpauloo/Documents/LevelSender/db/levelsender.sqlite`  
* clean that data (adjusting for barometric pressure)  
* push clean data to cloud SQL database  


*** 


# Clean SQLite Emails


First, load the relevant libraries.
```{r}
library(RSQLite)
library(DBI)
library(tidyverse)
library(stringr)
library(lubridate)
```


Refresh emails and append to sqlite database.
```{r}
# simulates clicking the "Retrieve Emails Button"
system(
  shQuote(
    "C:/Program Files (x86)/Solinst/LevelSender/LSEmailClient.exe", # file path of email client .exe
    type = "cmd" # change to "sh" for Unix/bash, and "csh" for C-shell
  )
)
```


Connect to .sqlite database, and list the tables of data within.
```{r}
# connect to SQLite database that Solinist regularly updates
db <- dbConnect(SQLite(), dbname = "C:/Users/rpauloo/Documents/LevelSender/db/levelsender.sqlite")

# get metadata about level senders
meta <- dbReadTable(db, "LevelSenderSettings")

# read emails, select interesting data, filter for relevant emails with data, and rename columns
d <- dbReadTable(db, "ReceivedEmail") %>% 
  select(ReceivedDate, Subject, Body) %>% 
  filter( grepl("LS Report", Subject) ) %>% 
  rename(date = ReceivedDate, subject = Subject, body = Body)

# We want to arrange these emails by the date they were received, but first we need to convert the `Date` from a character vector to a `Date` object.
d$date <- as.POSIXct( strptime( d$date, "%Y-%m-%d %H:%M:%S" ) )
```


Filter for all emails within a 30 day window of the current date.
```{r}
# 30 day rolling window
current <- Sys.Date() - 30

# add another date column without times
d$date_2 <- as_date(d$date)

# filter for all emails from `current` onwards
check <- filter(d, date_2 >= current)
```


Identify records containing MW5.
```{r}
# serial number for mw5 level sender
mw5 <- ": 283687"

# separate each email body into a vector of lines, and store each in a list element
lines <- lapply(check$body, function(x){unlist(strsplit(x, "\r\n"))} )

# function to apply
get_data <- function(v){ 
  
  # initalize baro vector
  baro <- NULL 
  
  # does mw5 appear in the email? 
  ss <- sum(str_detect(v, mw5)) 
  
  # if the well == MW 5
  if (ss == 1) {
    id <- v[str_detect(v, "Serial: ")][1]    # 1st serial is level sender id
    baro <- v[str_detect(v, "Serial: ")][3]  # 3rd serial is baro  logger id
  }

  # if the well != mw5
  if (ss == 0) {
    id <- v[str_detect(v, "Serial: ")][1]    # 1st serial number is level sender id
  }

  # subset for the level logger serial number by string position "Serial: #######"
  id <- as.numeric(substr(id, 9, nchar(id)))
  
  # if barometric pressure logger is present, get its serial
  if(!is.null(baro)){baro <- as.numeric(substr(baro, 9, nchar(baro)))}

  
  # if the well == MW 5
  if (ss == 1) {
    
    # starting and ending index of logger 1 (monitoring well)
    mw_0 <- str_which(v, "Logger 1 Samples") + 2
    mw_n <- str_which(v, "Logger 2 Samples") - 2
    
    # starting and ending index of logger 2 (baro logger)
    bl_0 <- str_which(v, "Logger 2 Samples") + 2
    bl_n <- str_which(v, "MESSAGES: Email report") - 2
    
    # organize into a dataframe
    v1 <- v[mw_0:mw_n]                           # monitoring well lines 
    v2 <- v[bl_0:bl_n]                           # barologger lines
    m1 <- str_split_fixed(v1, ", ", 3)           # matrix of mw strings
    m2 <- str_split_fixed(v2, ", ", 3)           # matrix of baro strings
    m1[, 2:3] <- round(as.numeric(m1[, 2:3]), 2) # round temp and level
    m2[, 2:3] <- round(as.numeric(m2[, 2:3]), 2) # round temp and level
    
    df <- rbind.data.frame(m1, m2)               # convert to df
    colnames(df) <- c("dt", "temp", "level")     # rename columns
    df$dt <- dmy_hms(df$dt)                      # format dates
    
    # add ids
    df$id <- rep(c(id, baro), times = c(length(v1), length(v2)))
    
    # finagle the object classes
    df$temp <- as.numeric(levels(df$temp)[df$temp])
    df$level <- as.numeric(levels(df$level)[df$level])
    #df$id <- factor(df$id)
    
  }

  # if the well != mw5
  if (ss == 0) {
    
    # starting and ending index of logger 1 (monitoring well)
    mw_0 <- str_which(v, "Logger 1 Samples") + 2
    mw_n <- str_which(v, "MESSAGES: Email report") - 2
    
    
    # organize into a dataframe
    v1 <- v[mw_0:mw_n]                           # monitoring well lines 
    m1 <- str_split_fixed(v1, ", ", 3)           # matrix of mw strings
    m1[, 2:3] <- round(as.numeric(m1[, 2:3]), 2) # round temp and level
    
    df <- as.data.frame(m1)                      # convert to df
    colnames(df) <- c("dt", "temp", "level")     # rename columns
    df$dt <- dmy_hms(df$dt)                      # format dates
    
    # add ids
    df$id <- id
    
    # finagle the object classes
    df$temp <- as.numeric(levels(df$temp)[df$temp])
    df$level <- as.numeric(levels(df$level)[df$level])
    #df$id <- factor(df$id)
  }
  
  return(df)
}

# 
#v = lines[[1]] # non-MW5
# v = lines[[6]]  # MW5
# v = lines[[195]] # MW5 non-equal ... need to save extra baro without filtering it out
# another one is broken between 180 and 195... find it!
# get_data(v)
```


Apply function to all `lines` from `current` data.
```{r}
# apply function to list of current emails
dfs <- lapply(lines, get_data) # temp until non-equal issue is fixed

# bind all dfs together
all <- do.call(rbind, dfs)

# omit erronous values input when a measurement error is made
all <- filter(all, level < 1000)
```


Temporary--delete after Andrew replaces LS batteries. Then go into the database and fix these values... or somehow only append data that's new, while keeping the old "golden" data in the Cloud SQL db.
```{r}
#all <- filter(all, dt < ymd_hms("2018-10-18 00:00:00 UTC"))
```

Outage Hot Fixes. This will unfortunately be a messy part of the data cleaning pipeline.    

Battery outages in the LevelSender throw impossible values at the temp and level, e.g. - level = 0.00. Never let battery outages in the LS happen again because this hot fix actually takes some time. It requires downloading the Level Logger data, and manually inserting it into the database. When this does unfortunately happen you should:  

IN THE FIELD:  

1. Immediately change the batteries in the dead LS. Reported values will be noticably low and impossible (e.g. - level = -0.1 or 0.0).  

***  

IN THIS CLEANING SCRIPT (only after the field is done):  

2. Identify the start and end times of the outage, and omit from the data in this script.  
3. Separately, extract this same data range from the levelloggers, and upload into the script to replace the now extracted data with impossible values caused by the outage.  
4. Repeat for every new battery outage.  

```{r}
# October 2018 MW5 OUTAGE with affected the baro and water level loggers
out_ls_id <- c(283687, 2038232) # ids of the baro and water 

# Identify the start and end times of the outage, and omit from the data in this script. 
all %>% filter(id %in% out_ls_id) %>% arrange(dt) 

# define the outage window
# find the start and end times of the outage 
# (I went for the entire 24 hour span to make it easier to undertand in terms of days out)
outage_start <- ymd_hms("2018-10-18 00:00:00") # outage start
outage_end   <- ymd_hms("2018-10-29 23:00:00") # outage end

# omit the records in the window of the outage
all <- all %>% 
  filter( !(id %in% out_ls_id & dt >= outage_start & dt <= outage_end) )

# 
```



Separate barologger and monitoring well data. 
```{r}
# serial number for baro logger
baro_serial <- "2038232"

# barometric timeseries
baro_data <- filter(all, id == baro_serial)  

# monitoring well timeseries
mw_data <- filter(all, id != baro_serial) 

# monitoring well timeseries with corresponding barometric data
mw_data <- filter(mw_data, dt %in% baro_data$dt)
```


Visualize me and baro timeseries. Can delete once app is running.
```{r}
# visualize
mw_data %>% ggplot(aes(dt, level, color = factor(id))) + geom_line()

baro_data %>%
  #filter(level > 0) %>%
  ggplot(aes(dt, level, color = factor(id))) + geom_line()
```


***  


# Tranformations


Remove temperature from barometric and monitoring well data.
```{r}
baro_data <- select(baro_data, -temp)
mw_data   <- select(mw_data,   -temp)
```


Convert barometric data from PSI to meters.
```{r}
# PSI to meters conversion factor
psi_to_m <- function(psi){
  return(psi * 0.703070)
}

# convert barometric timeseries from PSI to meters 
baro_data$level <- psi_to_m(baro_data$level)
```


Adjust monitoring well levels by barometric data.
```{r}
# rename columns in baro and mw data to remove ambiguity, drop id in baro data
baro_data <- rename(baro_data, level_baro = level) %>% select(-id)
mw_data   <- rename(mw_data,   level_mw = level,   ls_id = id)

# join baro and mw databy datetime, calculate adjusted water level
adj_data <- left_join(baro_data, mw_data, by = "dt") %>% 
  mutate(adj_level = level_mw - level_baro) 
```


Adjust by elevation.
```{r}
# read in elevation data from github
elev <- read_csv("C:/Users/rpauloo/Documents/GitHub/cosumnes_shiny/clean/dependencies/elev.csv")
#elev <- read_csv("https://raw.githubusercontent.com/richpauloo/cosumnes_shiny/master/clean/andrew_clean_data/elev.csv")

# find baro-adjusted water level for matching date time in manually measured `date_of_last_dtw_measurement`

ls_ids <- unique(mw_data$ls_id) # vector of LS ids
  
calc_cable_length <- function(x){
  i     <- which(elev$ls_id == x)           # row index of level sender
  dolm  <- mdy_hm(elev$date_of_last_dtw_measurement[i])  # date of last measurement for LS
  dolml <- filter(adj_data, ls_id == x & dt <= dolm)[1, "adj_level"] # level at dolm
  return(dolml)
}

# combine ls ids and water levels at measurement date
key <- data.frame(ls_id = ls_ids,
                  wls   = sapply(ls_ids, calc_cable_length))


# calculate effective cable length from measured depth to water (every 3 months) 
# and actual water level above transducer at the date when water was measured ^^
# (still needs to be adjusted by 9.5m)
temp <- left_join(elev, key, by = "ls_id") %>% 
  mutate(cable_length = dtw_m + wls)


# updat adjusted data
adj_data <- left_join(adj_data, temp, by = "ls_id") 

adj_data <- mutate(adj_data, adj_level = ifelse(ll_subtracts_95_m == TRUE, 
                                                9.5 + adj_level, 
                                                adj_level))


# add water elevation and subtract cable length to find final water level
adj_data <- mutate(adj_data, final_level = adj_level + elev_m - cable_length) 
```


Visualize final level. Remove later.
```{r}
adj_data %>% ggplot(aes(dt, final_level, color = factor(ls_id))) + geom_line()
```


Connect to cloud database. 
```{r}
# password for gw obs db
pw <- read_rds("C:/Users/rpauloo/Documents/GitHub/cosumnes_shiny/dashboard/data/pw.rds")

# connect to the cloud SQL database 
cdb <- dbConnect(RMySQL::MySQL(),
                 user = "gw_observatory",
                 password = pw,
                 host = "169.237.35.237",
                 dbname = "gw_observatory",
                 port = 33306)
```


Read the elevation table, and adjust well levels by their elevation.
```{r}
elev <- dbReadTable(cdb, "elevation_data")

```


***

# Write Transformed Data to Cloud DB

Read the present data, define classes, append new data, and filter for the unique rows.
```{r}
# most current data
present <- dbReadTable(cdb, "present")

# fix class of dates and levels in present data
present$date <- ymd_hms(present$date)
present <- present %>% 
  gather(well, level, -date) %>% 
  mutate(level = as.numeric(level)) %>% 
  spread(well, level)

# convert variables into correct classes
gather(present, well, level, -Date) %>%  
  rename(date = Date) %>%                 # rename date column
  mutate(date = mdy_hm(date),             # convert date to datetime
         level = as.numeric(level)) %>%   # convert level to numeric
  filter(level < 1000)                    # filter erroneous large values

# append new data
rbind()

# gather 



```


Overwrite the `present` data in the cloud database.  
```{r}

```


Save a version of the database every 7 days.
```{r}
# days of year to save copy of the data
save_days <- seq(8,365, 7)

# get day of the year
doy <- yday(Sys.Date())

# if present day is a save day, save a version of the database
if(doy %in% save_days == TRUE){
  dbWriteTable(cdb, 
               name = Sys.Date(),
               value = appended_unique)
}
```


Save this data to the cloud databse.
```{r}
dbWriteTable(cdb, 
             name = "present",
             value = appended_unique)
```




Disconnect from SQLite and cloud SQL databases.
```{r}
# disconnect
dbDisconnect(db)
dbDisconnect(cdb)
```


